{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0997f830",
   "metadata": {},
   "source": [
    "<p align= \"center\">\n",
    "<img width=\"300\" src=\"https://logos-world.net/wp-content/uploads/2020/11/GitHub-Emblem.png\" alt=\"Github Logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c4739",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align = \"center\">Programming Language Detection</h1>\n",
    "\n",
    "<h2 align = \"center\">By Chloe Whitaker, Jeanette Schulz, Brian Clements, and Paige Guajardo </h2>\n",
    "<h4 align = \"center\">11 February 2022</h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec8047",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "# About this Project\n",
    "### Github Webscraping and Natural Language Processing\n",
    "Millions of developers and companies build, ship, and maintain their software on GitHubâ€” the largest and most advanced development platform in the world. As Codeup's new up-and-coming Data Scientists, we will be using GitHub's platform to practice both our Web-Scraping skills and our Natural Language Processing (NLP) skills. With a focus on repositories that are studying bitcoin, our goal is to predict the programming language used in a repository based solely on the README.md file provided. By exploring the text provided in the README, we hope to identify key words that will allow us to identify which programming language(s) were used. Then we will teach these to our classification model so that it will predict the programming language of any future repositories we show it. For our project, we focused on five most common languages from Bitcoin repositories and named the rest 'Other'. The list of languages we will try to predict are: \n",
    "- JavaScript         \n",
    "- Python             \n",
    "- C++                 \n",
    "- PHP                 \n",
    "- C                                   \n",
    "- Java   \n",
    "- Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7bd3c",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "### Imports\n",
    "Here are the imports needed to run this notebook."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 1,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "640d9438",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'github_token' from 'env' (/Users/jeanette_schulz/codeup-data-science/NLP_Project/env.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bv/mlqqsh2x0wl7z5f5vcgxngd00000gn/T/ipykernel_51475/2074234611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# import acquire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwrangle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/NLP_Project/wrangle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgithub_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgithub_username\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'github_token' from 'env' (/Users/jeanette_schulz/codeup-data-science/NLP_Project/env.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.sentiment\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "# Time\n",
    "from time import strftime\n",
    "\n",
    "import unicodedata\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom Imports\n",
    "# import acquire \n",
    "import prepare\n",
    "import wrangle\n",
    "import model\n",
    "import explore\n",
    "\n",
    "# Turn off pink boxes for demo\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964180d",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "# Let's Get Started...\n",
    "\n",
    "<hr style=\"border:2px solid blue\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afd8fb",
   "metadata": {},
   "source": [
    "## Wrangle\n",
    "Use the wrangle.py helper file to acquire and prepare the GitHub README data. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "87e7a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=acquire.make_json(cached=True)\n",
    "df = pd.read_json('repo_readmes.json')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "id": "8acdf1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>using-system/LightningPay</td>\n",
       "      <td>C#</td>\n",
       "      <td># LightningPay\\nBitcoin Lightning Network Paym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drminnaar/react-bitcoin-monitor</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># React Bitcoin Monitor\\n\\nAn app that monitor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lbryio/lbrycrd</td>\n",
       "      <td>C++</td>\n",
       "      <td># LBRYcrd - The LBRY blockchain\\n\\n[![Build St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElementsProject/lightning-charge</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Lightning Charge\\n\\n[![build status](https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kilimchoi/cryptocurrency</td>\n",
       "      <td>None</td>\n",
       "      <td>Check out https://coinbuddy.co/coins to track ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               repo    language  \\\n",
       "0         using-system/LightningPay          C#   \n",
       "1   drminnaar/react-bitcoin-monitor  JavaScript   \n",
       "2                    lbryio/lbrycrd         C++   \n",
       "3  ElementsProject/lightning-charge  JavaScript   \n",
       "4          kilimchoi/cryptocurrency        None   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # LightningPay\\nBitcoin Lightning Network Paym...  \n",
       "1  # React Bitcoin Monitor\\n\\nAn app that monitor...  \n",
       "2  # LBRYcrd - The LBRY blockchain\\n\\n[![Build St...  \n",
       "3  # Lightning Charge\\n\\n[![build status](https:/...  \n",
       "4  Check out https://coinbuddy.co/coins to track ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "id": "8acdf1e5",
   "metadata": {},
   "outputs": [],
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "id": "7a586bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "id": "7a586bd2",
   "metadata": {},
   "outputs": [],
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "137ce388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the names of the top 5 programming languages, and change the rest to 'Other'\n",
    "df['language'] = df.language.apply(wrangle.common_language)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "cea30b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.brian_quick_clean(df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "d46601a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test = \\\n",
    "wrangle.split_repos(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3c083",
   "metadata": {},
   "source": [
    "### Steps Taken to Prepare the Data:\n",
    "All data cleaning was to the README text in each repo\n",
    "- Dropped duplicates\n",
    "- normalized text by making all text lowercase\n",
    "- tokenized the text\n",
    "- removed stop words\n",
    "- converted programming languages with less than 10 repos to 'Other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e041497",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "# Exploration\n",
    "### Initial Hypothesis\n",
    "Our intial thought is that a lot of the READMEs contain unique words that could help predict the language of the repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ae768",
   "metadata": {},
   "source": [
    "#### Initial Questions: \n",
    "    1. What are the most frequently occuring words in the readmes?\n",
    "    2. Are there words that uniquely identify with a certain language's repos?\n",
    "    3. What are the least frequently occuring words in the readmes?\n",
    "    4. Does the length of the README vary by programming language?\n",
    "    5. How many repos have their programming language mentioned in the README text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399d85a",
   "metadata": {},
   "source": [
    "### Explore: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb859b9",
   "metadata": {},
   "source": [
    "To further prepare the data for explore, we will combine all of the words from the readmes together so that we can explore the entire word set as well as by language. We will also remove the common stop words and represent the contents as word frequencies."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "b4f99ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all contents in single string by language\n",
    "\n",
    "javascript_words = (' '.join(train[train.language == 'JavaScript'].lemmatized))\n",
    "c_plus_plus__words = (' '.join(train[train.language == 'C++'].lemmatized))\n",
    "c_words = (' '.join(train[train.language == 'C'].lemmatized))\n",
    "python_words = (' '.join(train[train.language == 'Python'].lemmatized))\n",
    "php_words = (' '.join(train[train.language == 'PHP'].lemmatized))\n",
    "java_words = (' '.join(train[train.language == 'Java'].lemmatized))\n",
    "other_words = (' '.join(train[train.language == 'Other'].lemmatized))\n",
    "all_words = (' '.join(train.lemmatized))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "c98b64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopwords\n",
    "\n",
    "javascript_words = wrangle.remove_stopwords(str(javascript_words))\n",
    "c_plus_plus__words = wrangle.remove_stopwords(str(c_plus_plus__words))\n",
    "c_words = wrangle.remove_stopwords(str(c_words))\n",
    "python_words = wrangle.remove_stopwords(str(python_words))\n",
    "java_words = wrangle.remove_stopwords(str(java_words))\n",
    "php_words = wrangle.remove_stopwords(str(php_words))\n",
    "other_words = wrangle.remove_stopwords(str(other_words))\n",
    "all_words = wrangle.remove_stopwords(str(all_words))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "fdf68618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent contents as word frequencies\n",
    "\n",
    "javascript_freq = pd.Series(javascript_words.split()).value_counts()\n",
    "c_plus_plus_freq = pd.Series(c_plus_plus__words.split()).value_counts()\n",
    "c_freq = pd.Series(c_words.split()).value_counts()\n",
    "python_freq = pd.Series(python_words.split()).value_counts()\n",
    "java_freq = pd.Series(java_words.split()).value_counts()\n",
    "php_freq = pd.Series(php_words.split()).value_counts()\n",
    "other_freq = pd.Series(other_words.split()).value_counts()\n",
    "all_freq = pd.Series(all_words.split()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426d912",
   "metadata": {},
   "source": [
    "We will concatinate all of the word frequencies together into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": null,
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "id": "6da9ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all frequencies together into a dataframe\n",
    "\n",
    "word_counts = pd.concat([javascript_freq, c_plus_plus_freq, c_freq, python_freq, java_freq, php_freq, other_freq, all_freq], axis=1).fillna(0).astype(int)\n",
    "word_counts.columns = ['javascript', 'c_plus_plus', 'c', 'python', 'java', 'php', 'other', 'all']\n",
    "# word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c53592",
   "metadata": {},
   "source": [
    "#### Question 1: What are the most frequently occuring words in the readmes?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "id": "fcc27e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'octocat_logo.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5083afb8ab00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# word cloud of the most common words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexplore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgithub_logo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/NLP_Project/explore.py\u001b[0m in \u001b[0;36mgithub_logo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# get image file and smecipy wordcloud parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"octocat_logo.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontour_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontour_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'octocat_logo.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "id": "fcc27e62",
   "metadata": {},
   "outputs": [],
>>>>>>> 765e80d30a347fcc58d40e76017698daaf8d0ad1
   "source": [
    "# word cloud of the most common words \n",
    "explore.github_logo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequently occuring words: sort by 'all'\n",
    "word_counts.sort_values('all', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869d959",
   "metadata": {},
   "source": [
    "#### Question 2: Are there words that uniquely identify with a certain language's repos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d3111",
   "metadata": {},
   "source": [
    "JavaScript contains the highest precentage of the 20 most common words used in the readmes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43780f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(word_counts.sort_values('all', ascending=False)\n",
    " .head(20)\n",
    " .apply(lambda row: row/row['all'], axis = 1)\n",
    " .drop(columns = 'all')\n",
    " .sort_values(by = 'javascript')\n",
    " .plot.barh(stacked = True, width = 1, ec = 'k')\n",
    ")\n",
    "plt.title('% of JavaScript vs All for the most common 20 words')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fce43",
   "metadata": {},
   "source": [
    "The most common words by language: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea30478",
   "metadata": {},
   "outputs": [],
   "source": [
    "javascript_freq.idxmax(), java_freq.idxmax(), c_plus_plus_freq.idxmax(), c_freq.idxmax(), python_freq.idxmax(), java_freq.idxmax(), php_freq.idxmax(), other_freq.idxmax(), all_freq.idxmax()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c114d3",
   "metadata": {},
   "source": [
    "#### Question 3: What are the least frequently occuring words in the readmes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequently occuring words: sort by 'all'\n",
    "word_counts.sort_values('all', ascending=False).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21559ff",
   "metadata": {},
   "source": [
    "* We really dont see any key info lets see with a graph now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1fb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rc('font', size=15)\n",
    "word_counts.sort_values('all', ascending=False).tail(20)[['javascript',\n",
    "'c_plus_plus',\n",
    "'c',\n",
    "'python',\n",
    "'java',\n",
    "'php',\n",
    "'other'                                                    \n",
    "]].plot.barh()\n",
    "plt.title('Programming Language for the top 20 most unfrequent words')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115fc2a",
   "metadata": {},
   "source": [
    "* Alot of these words seem to be tied with only C++, lets take a look finally at the bottom words of all the langauges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab974887",
   "metadata": {},
   "outputs": [],
   "source": [
    "javascript_freq.idxmin(), java_freq.idxmin(), c_plus_plus_freq.idxmin(), c_freq.idxmin(), python_freq.idxmin(), java_freq.idxmin(), php_freq.idxmin(), other_freq.idxmin(), all_freq.idxmin()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f369991",
   "metadata": {},
   "source": [
    "#### Question 4: Does the length of the README vary by programming language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca72cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length_list =df.groupby('language')\\\n",
    ".apply(lambda x: x.lemmatized.str.len().mean())\n",
    "# .apply(lambda x: x/len(df[df.language]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694499b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for index in length_list.index:\n",
    "    print(f\"{index}'s average repo length is {length_list[i]/len(df[df.language==index])}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887afb55",
   "metadata": {},
   "source": [
    "- The language with the longest repos, by far, is C (using the current dataset)\n",
    "- Outside of Java, the rest of the repos have an average length of roughly 200 or so lemmatized words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692674f",
   "metadata": {},
   "source": [
    "#### Question 5: In the README's, is the programming language used in the repo mentioned at all? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3240092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the word 'java' ever mentioned in a Java README?\n",
    "java_words.find(' java ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the word 'javascript' ever mentioned in a JavaScript README?\n",
    "javascript_words.find(' javascript ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ce86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the word 'python' ever mentioned in a Python README?\n",
    "python_words.find(' python ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04479f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the word 'c' ever mentioned in a C++ README?\n",
    "# Keep in mind 'c++' would be turned into 'c' after cleaning\n",
    "c_plus_plus__words.find(' c ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ea1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the word 'php' ever mentioned in a PHP README?\n",
    "php_words.find(' php ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the word 'c' ever mentioned in a C README?\n",
    "c_words.find(' c ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303cba8",
   "metadata": {},
   "source": [
    "Answer: It's not very common to mention the programming language used it seems. Java has zero README's that mention their use. As for JavaScript, Python, PHP, C++, and C: these languages are mentioned at least once in one of thier repos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e1572",
   "metadata": {},
   "source": [
    "### Exploration Takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a32924",
   "metadata": {},
   "source": [
    "- JavaScript contains the highest percentage of the 20 most common words used in the readmes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f22045",
   "metadata": {},
   "source": [
    "### Features to Move Forward with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689f741",
   "metadata": {},
   "source": [
    "For features, we are going to use the TF-IDF Method to provide numeric values that we can use to model on when we pass it the lemmatized README contents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09df811",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3d1a8",
   "metadata": {},
   "source": [
    "### Establishing the Baseline\n",
    "The last thing we have to do before fitting (training) out models is establishing a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11891cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the baseline, look at the value counts\n",
    "train.language.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the mode of the value counts\n",
    "baseline = y_train.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_bsl_prediction = y_train == 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eda48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = match_bsl_prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basline accuracy = 31%\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407e0a5",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "### Our best-performing model without undue overfitting was a Logistic Regression model\n",
    "\n",
    "### We ran over 10 models, between a Bag-of-Words approach and a TF-IDF approach\n",
    "<hr style=\"border:2px solid blue\"> </hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335c42b",
   "metadata": {},
   "source": [
    "## TF-IDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df.lemmatized)\n",
    "y = df.language\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, stratify=y, \\\n",
    "                                                            test_size=.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train,stratify=y_train, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071470b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train['lr_predicted_tdidf'] = lm.predict(X_train)\n",
    "validate['lr_predicted_tdidf'] = lm.predict(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2db0e",
   "metadata": {},
   "source": [
    "### Confustion matrix, train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79904b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.print_lr_tfidf_model_train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332ebf7",
   "metadata": {},
   "source": [
    "### Confustion matrix, validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17570c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_lr_tfidf_model_validate(validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bc177",
   "metadata": {},
   "source": [
    "## TF-IDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b426411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6db86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df.lemmatized)\n",
    "y = df.language\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, stratify=y, \\\n",
    "                                                            test_size=.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train,stratify=y_train, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train['lr_predicted_bagofwords'] = lm.predict(X_train)\n",
    "validate['lr_predicted_bagofwords'] = lm.predict(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46b99d",
   "metadata": {},
   "source": [
    "### Confustion matrix, train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524f22f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.print_lr_bagofwords_model_train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2387635",
   "metadata": {},
   "source": [
    "### Confustion matrix, validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_lr_bagofwords_model_validate(validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea2105",
   "metadata": {},
   "source": [
    "## Our best-performing model, with the least difference between train and validate, was the TF-IDF data on the Linear Regression model; we will run it on the Test data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "test = pd.DataFrame(dict(actual=y_test))\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "test['lr_predicted'] = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(test.actual, test.lr_predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(test.lr_predicted, test.actual))\n",
    "print('---')\n",
    "print(classification_report(test.actual, test.lr_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f579c0",
   "metadata": {},
   "source": [
    "## Our final result, running our best model on the test data, was 53% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840cf61",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>\n",
    "\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df561680",
   "metadata": {},
   "source": [
    "- Based on the data we ultimately ran through the model, a convincing prediction of the repo's language proved somewhat elusive.\n",
    "- Further iterations of the data manipulation would yield better results--exploring stopwords, for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c78acb",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd2090",
   "metadata": {},
   "source": [
    "- Our model produced modest results, but the data yielded an interesting NLP exploration and modeling.  Data manipulation was tricky at times, as were aspects of acquiring the data from the Github website, making interpretation elusive at times. Still, we are happy to have been able to extract some interesting aspects of the different languages and worth-while visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3277fe",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f74d8",
   "metadata": {},
   "source": [
    "- Explore stopwords (whose elimination might yield a more predictive set of words for the models)\n",
    "- Run on more model types, and experiment further with hyper-parameters\n",
    "- Further exploration--especially a network graph to illuminate the relationship of one language to the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65107d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
